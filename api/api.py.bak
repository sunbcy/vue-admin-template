from flask import Flask
import time
import requests
from bs4 import BeautifulSoup
import re
import jsonify
from urllib.parse import urljoin, urlparse
from traceback import print_exc

app = Flask(__name__, static_folder='../../dist',template_folder="../../dist", static_url_path='/')

@app.route('/time')
def get_current_time():
    return {'time': time.time()}

@app.route("/")
def index():
  return app.send_static_file('index.html')

def get_page_info(url):
    try:
        # 发送http请求获取网页内容
        headers = {'Content-Type': 'application/json; charset=utf-8'}
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        title = soup.title.string if soup.title else None
        description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else None
        # 提取文本内容
        text_content = soup.get_text()

        # 正则表达式提取关键词
        keywords = re.findall(r'<meta\s*name=["\']keywords["\']\s*content=["\'](.*?)["\']\s*>', res.text)

        # 这里可以加入更多的信息提取逻辑
        return {
            'title': title,
            'description': description,
            'text_content': text_content,
            'keywords': keywords
        }
    except Exception as e:
        return {'error': str(e)}

def get_page_links(url):
    try:
        res = requests.get(url)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        links = [a['href'] for a in soup.find_all('a', href=True)]
        absolute_links = [urljoin(url, link) for link in links]
        new_domains = []
        for each_link in absolute_links:
            try:
                extracted_domain = each_link.split('://')[1].split('/')[0]
                if extracted_domain not in new_domains and extracted_domain != url.split('://')[1].split('/')[0]:
                    new_domains.append(extracted_domain)
            except IndexError as e:
                print(url)
                break
        print(new_domains)  # 输入的新URL包含的域名
        return absolute_links
    except Exception as e:
        print_exc(e)
        return {'error': str(e)}


@app.route('/get_links/<newurl>', methods=['GET'])
def get_links_from_url(newurl):
    if ('http://' in newurl or 'https://' in newurl):
        # page_info = get_page_info(newurl)
        page_links = get_page_links(newurl)
    else:
        # page_info = get_page_info('http://' + newurl)
        page_links = get_page_links('http://' + newurl)
    print(page_links)  # , page_info)
    return page_links


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
